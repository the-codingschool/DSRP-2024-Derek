---
title: "Predicting IMDB Ratings of The Office Episodes Based on Main Cast Sentiment Scores"
author: "Matthew Fin"
format: html
editor: visual
---

```{r, echo = FALSE}
library(dplyr)
library(ggplot2)
library(caTools)
library(readr)
library(tidyverse)
data <- read.csv("data/office_sentiment .csv")
data <- data |>
  rename('character_name' = character)
```

Abstract:

Problem: This study will explore how the emotion of characters in The Office, quantified through sentimentR, syuzhet and sentimentAnalysis scores, can predict IMDB ratings. Understanding this relationship is extremely valuable for being able to evaluate how sentiment can affect viewer perception and ratings.

Approach: In order to explore this problem, I determined the mean sentiment scores for every single available episode in the data set. Using this information, along with the IMDb ratings of each episode, allowed me to create a machine learning program which used linear regression to attempt to predict IMDB ratings based on each of the main character's sentiment scores. This program split the data into two groups, one of which it trained upon, the other was used as a test for the model.

Results: This approach led to me discover some of the main characters' (Jim and Andy) sentiments were not effective predictors on their own, which led me to find the strongest possible model, which was relatively strong compared to others, but overall a mostly weak model. Despite not having an strong R\^2 value, this model was well within the significant level, meaning this result is very significant even if each character is not necessarily significant on their own.

Conclusion: While sentiment scores do provide some value as predictors for IMDB rating they aren't perfect. However, due to their huge list of applications across all of media the value of sentiment scores as a predictor for audience reaction should not be understated.

Background:

Understanding how we, as consumer of media, can have our perception, represented in this case by IMDB rating, altered or predicted by the sentiment's of those within the media, has great value in that it asks us to appreciate the complexities ofpublic opinion. It highlights the need to approach media ratings with a critical eye and consider how emotion of characters may affect our own perception.

Research Question: To what extent can the IMDB rating of an episode of The Office be predicted by the main cast's sentiment scores?

Hypothesis: I believe that the results will reveal mean character sentiment scores throughout an episode to be a strong but imperfect predictor of an episode's IMDB rating.

Results:

Plot 1 & 2:

Mean IMDB Rating over Time

Mean SentimentR Score of Jim over Time

```{r, echo = FALSE}
ndata <- data%>%
  group_by(season, ) %>%
  summarise(mean_scores = mean(imdb_rating, na.rm = TRUE))
ggplot(ndata, aes(y = mean_scores, x= factor(season), group = 1)) +
  geom_point(color = "blue") +
  geom_line(color = "red", linetype = "dashed") +
  labs(title = "Mean IMDB Rating Over Time",
       x = "Season",
       y = "Mean IMDB Rating")
sdata <- data %>%
  group_by(character_name, season) %>%
  filter(character_name == "Dwight") %>%
  filter(sentimentr_score != 0) %>%
  summarise(mean_score = mean(sentimentr_score))
ggplot(sdata, aes(y = mean_score, x = factor(season), group = 1)) +
  geom_point(color = "blue") +
  geom_line(color = "red", linetype = "dashed") +
  labs(title = "Mean SentimentR Score of Jim Over Time",
       x = "Season",
       y = "Mean SentimentR Score")
```

First, in order to determine whether I believed this was a worthy research question to follow, I created two initial plots. These plots graph the mean IMDB rating of each season, and also Jim's mean sentimentr score for each season. Comparing these two, one can easily a somewhat similar trend with higher highs early on and a dip near the end. This led me to believe that these scores are in fact correlated, making sentiment scores a valuable predictor of IMDB rating.

Table 1:

Table of Total Character Lines

```{r, echo = FALSE}

summary_data <- data %>%
  group_by(character_name) %>%
  summarise(total_appearances = n()) %>%
  arrange(desc(total_appearances))
print(summary_data, n = 10)
```

This plot reveals the number of lines that each character has throughout the entire TV Show. Using this table, in addition, to wikipedia's The Office page, I determined which characters are considered to be the main characters. Using this information, I could then begin creating regressions to attempt to find how strong of a predictor sentiment scores can be.

Plot 3:

Main Cast Regression Plot

```{r, echo = FALSE}
result <- data %>%
  group_by(character_name, season, episode) %>% 
  filter(sentimentAnalysis_score != 0) %>%
  summarise(mean_value = mean(sentimentAnalysis_score)) 

resultr <- data %>%
  mutate(character_name_plus_2 = paste(character_name, "2", sep = ''))
resultr <- resultr %>%
  group_by(character_name_plus_2, season, episode) %>% 
  filter(sentimentr_score != 0) %>%
  summarise(mean_value = mean(sentimentr_score)) 

resultz <- data %>%
  mutate(character_name_plus_3 = paste(character_name, "3", sep = ''))
resultz <- resultz %>%
  group_by(character_name_plus_3, season, episode) %>% 
  filter(syuzhet_score != 0) %>%
  summarise(mean_value = mean(syuzhet_score)) 

df_wide <- result %>%
  pivot_wider(
    names_from = character_name,     
    values_from = mean_value,         
    names_sort = TRUE) %>%                 
  arrange(episode, season)

df_wider <- resultr %>%
  pivot_wider(
    names_from = character_name_plus_2,     
    values_from = mean_value,         
    names_sort = TRUE) %>%                 
  arrange(episode, season)

df_widez <- resultz %>%
  pivot_wider(
    names_from = character_name_plus_3,     
    values_from = mean_value,         
    names_sort = TRUE) %>%                 
  arrange(episode, season)


merged_df <- df_wide %>%
  full_join(df_wider) %>%
  full_join(df_widez)

results <- data %>%
  group_by(season, episode, imdb_rating) %>%
  distinct(season, episode)


merged_df$imdb_rating <- paste(results$imdb_rating)
merged_df$imdb_rating <- as.numeric(merged_df$imdb_rating)
set.seed(42)
datas <- merged_df[, c("imdb_rating", "Jim", "Jim2", "Jim3", "Dwight", "Dwight2", "Dwight3", "Michael", "Michael2", "Michael3", "Pam", "Pam2", "Pam3", "Andy", "Andy2", "Andy3", "season")]
split <- sample.split(datas$imdb_rating, SplitRatio = 0.8)
train_data <- subset(datas, split == TRUE)
test_data <- subset(datas, split == FALSE)
lr_model <- lm(imdb_rating ~ Michael + Michael2 + Michael3 + Dwight + Dwight2 + Dwight3 + Pam + Pam2 + Pam3 + Jim + Jim2 + Jim3 + Andy + Andy2 + Andy3 , data = train_data)
pred <- predict(lr_model, newdata = test_data)
ggplot(test_data, aes(x = imdb_rating, y = pred, color = factor(season))) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Real v.s. Predicted Values (All Seasons)",
       x = "True IMDB Rating",
       y = "Predicted IMDB Rating",
       color = "Season",
  ) +
  scale_x_continuous(
    limits = c(6,10)
  ) +
  scale_y_continuous(
    limits = c(6,10)
  )
summary(lr_model)
```

This plot graphs the computer's predictions of IMDB ratings, based off the episodes it was given in its training data, against the actual IMDB scores of the test data. Using this plot I was also able to determine the R^2^ , R\^2 Adjusted, and P Value of this graph, revealing this to a significant, but relatively weak result. Additionally, using this model also allowed me to break down the predictors into individual characters as well as individual seasons.

Table 2:

Table of R^2^ and R\^2 Adj. values given by plot 3 by season and character

|        |                      |                        |                        |                        |                         |
|------------|------------|------------|------------|------------|------------|
| Season | Jim                  | Dwight                 | Michael                | Pam                    | Andy                    |
| 2      | 0.207/0.048          | 0.03857/-0.1537        | 0.02686/-0.1678        | 0.06998/-0.116         |                         |
| 3      | 0.0988/-0.0814       | 0.368/0.2415           | 0.04036/-0.1516        | 0.09147/-0.09024       | 0.181/-0.09206          |
| 4      | 0.4027/0.145         | 0.7218/0.6025          | 0.2782/-0.03117        | 0.206/-0.1343          | 0.3669/0.05031          |
| 5      | 0.174/0.036          | 0.2531/0.1287          | 0.04666/-0.1122        | 0.1677/0.02902         | 0.06954/-0.1165         |
| 6      | 0.17/-0.007          | 0.2319/0.07825         | 0.1788/0.0146          | 0.01947/-0.1906        | 0.1394/-0.05919         |
| 7      | 0.173/0.0077         | 0.0442/-0.147          | 0.266/0.1087           | 0.08615/-0.09661       | 0.3475/0.217            |
| 8      | 0.134/-0.0389        | 0.2369/0.08425         |                        | 0.2672/0.06736         | 0.1599/-0.008083        |
| 9      | 0.044/-0.16          | 0.2118/0.04289         |                        | 0.1985/0.02672         | 0.07944/-0.2658         |
| ALL    | 0.0026/-0.018/0.9427 | 0.0646/0.04551/0.01989 | 0.02963/0.00164/0.3701 | 0.0547/0.03486/0.04456 | 0.01234/-0.01388/0.7033 |

Using plot 3's code, I was able to gather data for the following table. This table reveals which characters and seasons have or do not have predictive value as a predictor for IMDB rating. Using this information I will be able to create the best possible model using the main cast.

Plot 4:

Best Predictive Model:

```{r, echo = FALSE}
set.seed(42)
datas <- merged_df[, c("imdb_rating", "Jim", "Jim2", "Jim3", "Dwight", "Dwight2", "Dwight3", "Michael", "Michael2", "Michael3", "Pam", "Pam2", "Pam3", "Andy", "Andy2", "Andy3", "season")]
split <- sample.split(datas$imdb_rating, SplitRatio = 0.8)
train_data <- subset(datas, split == TRUE)
test_data <- subset(datas, split == FALSE)
lr_model <- lm(imdb_rating ~ Michael + Michael2 + Michael3 + Dwight + Dwight2 + Dwight3 + Pam + Pam2 + Pam3 + Andy + Andy2 + Andy3 , data = train_data)
pred <- predict(lr_model, newdata = test_data)
ggplot(test_data, aes(x = imdb_rating, y = pred, color = factor(season))) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Real v.s. Predicted Values (All Seasons)",
       x = "True IMDB Rating",
       y = "Predicted IMDB Rating",
       color = "Season",
  ) +
  scale_x_continuous(
    limits = c(6,10)
  ) +
  scale_y_continuous(
    limits = c(6,10)
  )
summary(lr_model)
```

Using information from table 2 allowed me to determine which characters are most likely to be negatively affecting our model. Using machine learning once again, we can test each of these models to determine the strongest model uses the entire main cast, outside of Jim, as predictors. However, the entire main cast is not much worse of a predictor than the above model.

Discussion:

The above results allow us to determine that, although thoroughly flawed, sentiment scores are a valuable predictor of IMDB ratings. A number of shortcomings of the data may have negatively affected our results, namely a large chunk of data that was missing from the middle seasons of the data set. Due to a number of episodes being completely missing from the data set, the machine had less data to learn from and to practice on, which would most likely harm its predictive value. In spite of this shortcoming, this result still has significant value to us as it somewhat confirms our hypothesis while also revealing to us that sentiment scores in some way are correlated to IMDB ratings meaning sentiment scores of other things, such as speeches, novels, or films can also be used as a possible predictor of audience reaction. I would like to further research this by moving away from the screen and possibly to speeches or novels, however, for the former it may be difficult to find a numeric representation of audience reaction. Lastly, although some questions remain, such as how the additional data may have strengthened or weakened my model, I believe this research to have been extremely successful overall.

Github:

<https://github.com/the-codingschool/DSRP-2024-Derek/tree/dev-Matthew>

Data:

This dataset was an amended version of the below database, in which my mentor, Derek, added the three sentiment scores as columns.

<https://github.com/bradlindblad/schrute>

\
**Acknowledgements**:

I would like to thank my mentor, Derek, for his continued guidance, inspiration and assistance throughout my research project.

I would like to thank my TAs, Shruti and Renate, for their help throughout the entire program.

I would like to thank my groupmates for their inspiration and feedback in my research.

I would like to thank The Coding School for this opportunity and everything I have learned in this wonderful program.
